{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small example to bechmark the perfromance of torchtuples vs regular pytorch\n",
    "\n",
    "In this notebook we illustrate the performance difference using the torchtuples Model object compared to regular pytorch, when we are working with small data sets in memory.\n",
    "\n",
    "The main difference is that the DataLoader provided by pytorch reads batches in a for-loop, which is somewhat slow.\n",
    "This is not the case for for larger network, as the DataLoader is no longer the bootleneck. \n",
    "However for smaler networks we see a significant difference.\n",
    "\n",
    "For the regulrar pytorch implementation (see fit_torch below), we use a standard TensorDataset and Dataloader for batched iterations. The Dataloader is not targeted towards small data sets in memory, so this comparison is somewhat unfair. It's not har do drop the data loader and write something faster on your own. However, the implementation in (fit_torch below), is quite common to find (e.g. in skorch https://skorch.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook is run on a 2016 MacBook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchtuples\n",
    "from torchtuples import Model, tuplefy\n",
    "from torchtuples.practical import MLPVanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtuples.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification # to create a data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu' # change to run on gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make both numpy and torch version of the data set, as Model also works with numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(n):\n",
    "    inp, tar = make_classification(n)\n",
    "    inp = inp.astype('float32')\n",
    "    tar = tar.reshape(-1, 1).astype('float32')\n",
    "    inp_tensor = torch.from_numpy(inp)\n",
    "    tar_tensor = torch.from_numpy(tar)\n",
    "    return inp, tar, inp_tensor, tar_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a vanilla mlp with two hidden layers, each with 64 nodes. Larger networks would produce timing results that are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First with use of torchtuples.Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_torchtuples(inp, tar, epochs, batch_size, num_workers):\n",
    "    torch.manual_seed(0)\n",
    "    net = MLPVanilla(inp.shape[1], [64, 64], 1)\n",
    "    optimizer = torchtuples.optim.SGD(0.01)\n",
    "    model = Model(net, loss_func, optimizer, device=device)\n",
    "    model.fit(inp, tar, batch_size, epochs, verbose=False, num_workers=num_workers)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standar pytorch implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_torch(inp, tar, epochs, batch_size, num_workers):\n",
    "    \"\"\" We have used (more or less) the same setup for the regular \n",
    "    pytorch implementation as they do in scorch: \n",
    "    https://skorch.readthedocs.io/en/latest/user/tutorials.html\n",
    "    \"\"\"\n",
    "    torch.manual_seed(0)\n",
    "    net = MLPVanilla(inp.shape[1], [64, 64], 1)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    dataset = torch.utils.data.TensorDataset(inp, tar)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size, True, num_workers=num_workers)\n",
    "    for _ in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_pred = net(x)\n",
    "            loss = loss_func(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "inp, tar, inp_tensor, tar_tensor = make_dataset(2000)\n",
    "epochs = 50\n",
    "batch_size = 256\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check code\n",
    "\n",
    "First we just verify that both implementations produce the same weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_tt = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)\n",
    "\n",
    "net_t = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)\n",
    "\n",
    "assert all([(w_tt == w_t).all() for w_tt, w_t in zip(net_tt.parameters(), net_t.parameters())]), 'Not equal weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing trainig progress\n",
    "\n",
    "We fist illustrte that for small data sets, there is no point in spinning up multiple worker for the data loading.\n",
    "This is because we close down the workers after every epoch, and hence the cost of starting them again is not worth it.\n",
    "\n",
    "Note that because torch.utils.data.DataLoader use a for-loop to collect a batch, it takes substantially longer time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.42 s, sys: 428 ms, total: 3.85 s\n",
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.42 s, sys: 855 ms, total: 7.27 s\n",
      "Wall time: 3.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.15 s, sys: 1.81 s, total: 7.96 s\n",
      "Wall time: 3.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.98 s, sys: 1.91 s, total: 7.89 s\n",
      "Wall time: 4.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.81 s, sys: 2.17 s, total: 7.98 s\n",
      "Wall time: 3.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.46 s, sys: 2.26 s, total: 7.72 s\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Larger data set\n",
    "\n",
    "Next, for a larger data set, there is some benefit in using a dedicated worker in torch.utils.data.DataLoader.\n",
    "However, the start-up cost of two workers is still to high, and for the the torchtuples dataloader, even one is to high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "inp, tar, inp_tensor, tar_tensor = make_dataset(100000)\n",
    "epochs = 10\n",
    "batch_size = 256\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.6 s, sys: 3.5 s, total: 37.1 s\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 7.85 s, total: 1min 11s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.8 s, sys: 6.1 s, total: 43.9 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 6.23 s, total: 45.5 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.7 s, sys: 5.73 s, total: 43.4 s\n",
      "Wall time: 17.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torchtuples(inp, tar, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.3 s, sys: 5.99 s, total: 42.3 s\n",
      "Wall time: 19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = fit_torch(inp_tensor, tar_tensor, epochs, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just data loaders\n",
    "\n",
    "Finally, if we just look at the data loaders, we can see there is quite a substantial difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_tt = tuplefy(inp_tensor, tar_tensor).make_dataloader(batch_size, True, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = torch.utils.data.TensorDataset(inp_tensor, tar_tensor)\n",
    "dl_t = torch.utils.data.DataLoader(ds, batch_size, True, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.67 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(dl_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.84 ms ± 78.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(dl_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
